{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Question Answering with local persistence\n",
    "\n",
    "An example of using Chroma DB and LangChain to do question answering over documents, with a locally persisted database. \n",
    "You can store embeddings and documents, then use them again later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import VectorDBQA\n",
    "from langchain.document_loaders import PyPDFLoader, csv_loader, DirectoryLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process documents\n",
    "\n",
    "Load documents to do question answering over. If you want to do this over your documents, this is the section you should replace.\n",
    "\n",
    "Next we split documents into small chunks. This is so we can find the most relevant chunks for a query and pass only those into the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = DirectoryLoader('pdfs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfss = []\n",
    "csvs = []\n",
    "# def vactorDB():\n",
    "import os\n",
    "for file in os.listdir('pdfs/'):\n",
    "    if file.endswith((\".pdf\")):\n",
    "        file_path = os.path.join('pdfs/', file)\n",
    "        # print(file_path)\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        pages = loader.load()\n",
    "        pdfss.append(pages)\n",
    "    elif file.endswith((\".csv\")):\n",
    "        file_path = os.path.join('pdfs/', file)\n",
    "        # import pandas as pd\n",
    "        # df = pd.read_csv(file_path)\n",
    "        # df = df.to_json(orient='records', force_ascii=False, lines=True, default_handler=str)\n",
    "        loader = csv_loader.CSVLoader(file_path)\n",
    "        # print(loader)\n",
    "        pages = loader.load()\n",
    "        csvs.append(pages)\n",
    "        # csvs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pdfss + csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from api import apikey\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=apikey)\n",
    "for i in range(len(final)):\n",
    "    db = Chroma.from_documents(final[i], embeddings, persist_directory=\"mydb\")\n",
    "    print(i)\n",
    "\n",
    "# # db.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(pdfss))\n",
    "print(len(csvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbs = db.similarity_search('What materials and courses does the platform offer?')\n",
    "len(dbs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist the Database\n",
    "In a notebook, we should call `persist()` to ensure the embeddings are written to disk.\n",
    "This isn't necessary in a script - the database will be automatically persisted when the client object is destroyed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Database from disk, and create the chain\n",
    "Be sure to pass the same `persist_directory` and `embedding_function` as you did when you instantiated the database. Initialize the chain we will use for question answering."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask questions!\n",
    "\n",
    "Now we can use the chain to ask questions!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "When you're done with the database, you can delete it from disk. You can delete the specific collection you're working with (if you have several), or delete the entire database by nuking the persistence directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "def generate_prompt(prompt,system_prompt):\n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "I want you to act as Metutors platform Assistant, you job is to answer users query by using given context.\n",
    "    if user ask in Arabic language your answers must be in Arabic otherwise in english. use the given data to generate answer.\n",
    "    if you cant find any relevent information\n",
    "    inside the context just ask dont tell him you dont have the data given user send contact on support on support@metutors.com.\n",
    "        if user ask in Arabic language your answers should be in Arabic otherwise in english.\n",
    "    This is important that your answers should be in detailed.\n",
    "      use the given data to generate answer.\n",
    "      \n",
    "    \"\"\".strip()\n",
    "    return f\"\"\"\n",
    "    [INST] <>\n",
    "    {system_prompt}\n",
    "    <>\n",
    "\n",
    "    {prompt} [/INST]\n",
    "    \"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "I want you to act as Metutors platform Assistant, you job is to answer users query by using given context.\n",
    "if user ask in Arabic language your answers must be in Arabic otherwise in english. use the given data to generate answer.\n",
    "if you cant find any relevent information\n",
    "inside the context just ask dont tell him you dont have the data given user send contact on support on support@metutors.com.\n",
    "    if user ask in Arabic language your answers should be in Arabic otherwise in english.\n",
    "This is important that your answers should be in detailed.\n",
    "    use the given data to generate answer.\n",
    "\n",
    "    if user ask about any pricing find cost in the given context and tell him\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "llm.model_name = \"gpt-4-1106-preview\"\n",
    "template = generate_prompt(\n",
    "    \"\"\"\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\",\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    ")\n",
    "# load from disk\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "db3 = Chroma(persist_directory=\"db\", embedding_function=embeddings)\n",
    "db3.get()\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "# path = os.getcwd()+\"//new//\"\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db3.as_retriever(search_kwargs={\"k\": 2}),\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# loader = PyPDFLoader(\"/content/merged_output.pdf\")\n",
    "# documents = loader.load()\n",
    "\n",
    "# text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "# docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# db = Chroma.from_documents(db, embedding)\n",
    "\n",
    "def retrieve_combined_documents(query, max_combined_docs=2):\n",
    "    retriever = db.as_retriever(search_type=\"mmr\")\n",
    "\n",
    "    rev_doc = retriever.get_relevant_documents(query)\n",
    "    lim_rev_doc = rev_doc\n",
    "\n",
    "    docs = db.similarity_search(query)\n",
    "    lim_docs = docs\n",
    "\n",
    "    combined_docs = lim_rev_doc + lim_docs\n",
    "\n",
    "    return combined_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = retrieve_combined_documents('Bounce (Ocean Mall, Karachi) timing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8968"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(str(a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chroma-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c909e91d0cd7642213937968dfc91c71973575965f56cdcabb1e0b29abe5f7fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
